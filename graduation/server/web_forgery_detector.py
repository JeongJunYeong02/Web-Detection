"""
ì›¹ì‚¬ì´íŠ¸ ìœ„ë³€ì¡° íƒì§€ ì‹œìŠ¤í…œ (CodeBERT ì œì™¸ ë²„ì „)
- í…ìŠ¤íŠ¸ ìœ ì‚¬ë„
- DOM êµ¬ì¡° ë¹„êµ
- URL ìœ ì‚¬ë„ (ë¡œê·¸ì¸ í¼ ë¶„ì„ í¬í•¨)
- ë‚´ë¶€ ë§í¬ URL ì¶”ì¶œ ë° ê²€ì‚¬ (ê³ ë„í™”)
- iframe í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ê²€ì‚¬ (CSV ê¸°ë°˜)
"""

from bs4 import BeautifulSoup
from difflib import SequenceMatcher
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from zss import simple_distance, Node
from urllib.parse import urlparse, urljoin
import numpy as np
import re
import Levenshtein
import pandas as pd
from pathlib import Path

# --- CodeBERT (ì œê±°ë¨) ---
_HAS_CODEBERT = False

class WebForgeryDetector:
    """ì›¹ì‚¬ì´íŠ¸ ìœ„ë³€ì¡° ì¢…í•© íƒì§€ ì‹œìŠ¤í…œ"""
    
    # â˜… [ìˆ˜ì •] __init__ ìƒì„±ìì—ì„œ baseline_url ê¸°ë³¸ê°’ì„ ìˆ˜ì •
    def __init__(self, whitelist_csv_path, baseline_html=None, baseline_html_path=None, baseline_url="http://localhost:8000/normal.html#"):  # type: ignore
        """
        Args:
            whitelist_csv_path: í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ URLì´ ë‹´ê¸´ CSV íŒŒì¼ ê²½ë¡œ
            baseline_html: ì½”ë“œ ë‚´ì—ì„œ ì§ì ‘ ì§€ì •ëœ baseline HTML (ì˜µì…˜)
            baseline_html_path: íŒŒì¼ì—ì„œ baseline HTMLì„ ì½ì–´ì˜¬ ê²½ë¡œ (ì˜µì…˜)
            baseline_url: baseline HTMLì˜ ê¸°ë³¸ URL (ë„ë©”ì¸ ìœ ì‚¬ë„ ë¹„êµìš©)
        """
        self.whitelist_urls = set()
        self.whitelist_domains = set()
        
        if whitelist_csv_path:
            self.load_whitelist_from_csv(whitelist_csv_path)

        self.baseline_url = baseline_url
        self.baseline_html = None
        if baseline_html:
            self.baseline_html = baseline_html
        else:
            try:
                # __file__ (í˜„ì¬ íŒŒì¼) ê¸°ì¤€ìœ¼ë¡œ baseline_html_path íƒìƒ‰
                path = Path(baseline_html_path) if baseline_html_path else (Path(__file__).parent / "normal.html")
                if path.exists():
                    self.baseline_html = path.read_text(encoding="utf-8")
                    print(f"âœ… Baseline HTML loaded from: {path}")
                else:
                    print(f"âŒ [WARN] Baseline HTML not found at: {path}")
            except Exception as e:
                print(f"âŒ Failed to load baseline HTML: {e}")
    
    def load_whitelist_from_csv(self, csv_path):
        try:
            df = pd.read_csv(csv_path)
            if 'url' in df.columns:
                urls = df['url'].dropna()
            elif 'domain' in df.columns:
                self.whitelist_domains.update(df['domain'].dropna().astype(str).str.strip())
                urls = [] # ë„ë©”ì¸ë§Œ ë¡œë“œ
            else:
                urls = df.iloc[:, 0].dropna() # ì²« ë²ˆì§¸ ì—´
            
            for url in urls:
                url = str(url).strip()
                self.whitelist_urls.add(url)
                domain = self._extract_domain(url)
                if domain:
                    self.whitelist_domains.add(domain)
            
            print(f"âœ… í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ: {len(self.whitelist_urls)} URLs, {len(self.whitelist_domains)} domains")
        
        except FileNotFoundError:
            print(f"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        except Exception as e:
            print(f"âŒ CSV ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def is_url_whitelisted(self, url):
        if not url:
            return {'is_whitelisted': False, 'match_type': 'empty', 'matched_entry': None}
        url = url.strip()
        if url in self.whitelist_urls:
            return {'is_whitelisted': True, 'match_type': 'exact_url', 'matched_entry': url}
        domain = self._extract_domain(url)
        if domain:
            if domain in self.whitelist_domains:
                return {'is_whitelisted': True, 'match_type': 'exact_domain', 'matched_entry': domain}
            for whitelisted_domain in self.whitelist_domains:
                if domain == whitelisted_domain or domain.endswith('.' + whitelisted_domain):
                    return {'is_whitelisted': True, 'match_type': 'subdomain', 'matched_entry': whitelisted_domain}
        return {'is_whitelisted': False, 'match_type': 'not_found', 'matched_entry': None}
    
    def analyze_webpage(self, suspicious_html, legitimate_html, current_url, suspicious_url):
        # ê°œë³„ ì§€í‘œ ì„ ê³„ì‚°
        url_sim = self.calculate_url_similarity(current_url, suspicious_url)
        text_sim = self.calculate_text_similarity(suspicious_html, legitimate_html)
        dom_sim = self.calculate_dom_similarity(suspicious_html, legitimate_html)
        # â˜… [ìˆ˜ì •] CodeBERT ëŒ€ì‹  1.0 (100%) ë°˜í™˜
        semantic_score = self.calculate_semantic_similarity_codebert(suspicious_html, legitimate_html) 

        results = {
            'url_similarity': url_sim,
            'text_similarity': text_sim,
            'dom_similarity': dom_sim,
            'semantic_features': self.analyze_semantic_features(suspicious_html, legitimate_html),
            'semantic_similarity': semantic_score, # 1.0ì´ ë¨
            'internal_links': self.analyze_internal_links(suspicious_html, current_url),
            'iframe_analysis': self.analyze_iframes(suspicious_html, current_url),
            # â˜… [ì‹ ê·œ] ë¡œê·¸ì¸ í¼ ë¶„ì„ ì¶”ê°€
            'login_form_analysis': self.analyze_login_forms(suspicious_html, current_url),
            'scores': {
                'text': text_sim.get('cosine_similarity', 0.0),
                'dom': dom_sim.get('tree_edit_distance', 0.0),
                'semantic': semantic_score, # 1.0ì´ ë¨
                'url': url_sim.get('url_levenshtein', 0.0)
            },
            'risk_assessment': {}
        }
        results['risk_assessment'] = self.calculate_overall_risk(results)
        return results

    def analyze_with_baseline(self, suspicious_html, current_url):
        legit_html = self.baseline_html or ""
        # â˜… [ìˆ˜ì •] baseline_urlì„ suspicious_url ì¸ì ëŒ€ì‹  legitimate_url (ë¹„êµ ê¸°ì¤€ URL)ë¡œ ì‚¬ìš©
        return self.analyze_webpage(
            suspicious_html=suspicious_html,
            legitimate_html=legit_html,
            current_url=current_url, # í˜„ì¬ ì ‘ì†í•œ URL
            suspicious_url=self.baseline_url # ê¸°ì¤€ URL (ì˜ˆ: http://localhost:8000/normal.html#)
        )
    
    # ==================== 0. CodeBERT ì‹œë§¨í‹± ìœ ì‚¬ë„ (ì œê±°ë¨) ====================

    def calculate_semantic_similarity_codebert(self, html1: str, html2: str) -> float:
        """ â˜… [ìˆ˜ì •] CodeBERT ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (í˜ë„í‹° ë°©ì§€ë¥¼ ìœ„í•´ 1.0 ë°˜í™˜)"""
        return 1.0
        
    # ==================== 1. í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ====================
    
    def calculate_text_similarity(self, html1, html2):
        text1 = self._extract_text(html1)
        text2 = self._extract_text(html2)
        return {
            'jaccard_similarity': self._jaccard_similarity(text1, text2),
            'cosine_similarity': self._cosine_similarity_tfidf(text1, text2),
            'levenshtein_similarity': self._levenshtein_similarity(text1, text2)
        }
    
    def _extract_text(self, html):
        soup = BeautifulSoup(html, 'html.parser')
        for tag in soup(['script', 'style', 'meta', 'link']):
            tag.decompose()
        return soup.get_text(separator=' ', strip=True)
    
    def _jaccard_similarity(self, text1, text2):
        set1 = set(text1.lower().split())
        set2 = set(text2.lower().split())
        if not set1 or not set2: return 0.0
        intersection = set1.intersection(set2)
        union = set1.union(set2)
        return len(intersection) / len(union)
    
    def _cosine_similarity_tfidf(self, text1, text2):
        try:
            vectorizer = TfidfVectorizer()
            tfidf_matrix = vectorizer.fit_transform([text1, text2])
            return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        except Exception: return 0.0
    
    def _levenshtein_similarity(self, text1, text2):
        if len(text1) > 1000: text1 = text1[:1000]
        if len(text2) > 1000: text2 = text2[:1000]
        distance = Levenshtein.distance(text1, text2)
        max_len = max(len(text1), len(text2))
        return 1 - (distance / max_len) if max_len > 0 else 0.0
    
    # ==================== 2. DOM êµ¬ì¡° ë¹„êµ ====================
    
    def calculate_dom_similarity(self, html1, html2):
        return {
            'tree_edit_distance': self._tree_edit_distance(html1, html2),
            'structural_similarity': self._structural_similarity(html1, html2)
        }
    
    def _parse_dom_tree(self, html):
        soup = BeautifulSoup(html, 'html.parser')
        body = soup.body if soup.body else soup
        if not body: return Node("empty") # body íƒœê·¸ ë“±ì´ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš°
        return self._build_tree(body)
    
    def _build_tree(self, element, max_depth=10, current_depth=0):
        if current_depth >= max_depth:
            return Node(element.name or 'text')
        label = element.name or 'text'
        if element.name and element.get('id'):
            label += f"#{element.get('id')}"
        node = Node(label)
        if hasattr(element, 'children'):
            for child in element.children:
                if hasattr(child, 'name') and child.name:
                    node.addkid(self._build_tree(child, max_depth, current_depth + 1))
        return node
    
    def _tree_edit_distance(self, html1, html2):
        try:
            tree1 = self._parse_dom_tree(html1)
            tree2 = self._parse_dom_tree(html2)
            distance = simple_distance(tree1, tree2)
            max_size = max(self._count_nodes(tree1), self._count_nodes(tree2))
            similarity = 1 - (distance / max_size) if max_size > 0 else 0.0
            return similarity
        except Exception: return 0.0
    
    def _count_nodes(self, node):
        count = 1
        for child in node.children:
            count += self._count_nodes(child)
        return count
    
    def _structural_similarity(self, html1, html2):
        soup1 = BeautifulSoup(html1, 'html.parser')
        soup2 = BeautifulSoup(html2, 'html.parser')
        features1 = self._extract_structural_features(soup1)
        features2 = self._extract_structural_features(soup2)
        similarity_scores = []
        for key in features1.keys():
            if key in features2:
                diff = abs(features1[key] - features2[key])
                max_val = max(features1[key], features2[key])
                similarity_scores.append(1 - (diff / max_val) if max_val > 0 else 1)
        return np.mean(similarity_scores) if similarity_scores else 0.0
    
    def _extract_structural_features(self, soup):
        return {
            'div_count': len(soup.find_all('div')),
            'form_count': len(soup.find_all('form')),
            'input_count': len(soup.find_all('input')),
            'a_count': len(soup.find_all('a')),
            'depth': self._calculate_max_depth(soup.body if soup.body else soup)
        }
    
    def _calculate_max_depth(self, element, current_depth=0):
        if not element or not hasattr(element, 'children'):
            return current_depth
        max_child_depth = current_depth
        for child in element.children:
            if hasattr(child, 'name') and child.name:
                depth = self._calculate_max_depth(child, current_depth + 1)
                max_child_depth = max(max_child_depth, depth)
        return max_child_depth
    
    # ==================== 3. ì˜ë¯¸ ê¸°ë°˜ ë¶„ì„ (í‚¤ì›Œë“œ) ====================
    
    def analyze_semantic_features(self, html1, html2):
        features1 = self._extract_semantic_features(html1)
        features2 = self._extract_semantic_features(html2)
        return {
            'suspicious_features': features1,
            'legitimate_features': features2,
            'feature_match_score': self._compare_features(features1, features2)
        }
    
    def _extract_semantic_features(self, html):
        soup = BeautifulSoup(html, 'html.parser')
        text = soup.get_text().lower()
        return {
            'has_login_form': bool(soup.find('input', {'type': 'password'})),
            'has_urgent_keywords': self._check_keywords(text, ['ê¸´ê¸‰', 'ì¦‰ì‹œ', 'ì°¨ë‹¨', 'ì •ì§€', 'urgent', 'immediate', 'suspended']),
            'password_input_count': len(soup.find_all('input', {'type': 'password'}))
        }
    
    def _check_keywords(self, text, keywords):
        return any(keyword in text for keyword in keywords)
    
    def _compare_features(self, features1, features2):
        matching_features = sum(1 for key in features1.keys() if features1[key] == features2[key])
        total_features = len(features1)
        return matching_features / total_features if total_features > 0 else 0.0
    
    # ==================== 4. URL ìœ ì‚¬ë„ ====================
    
    def calculate_url_similarity(self, url1, url2):
        return {
            'domain_match': self._check_domain_match(url1, url2),
            'url_levenshtein': self._url_levenshtein_similarity(url1, url2),
            'suspicious_patterns': self._check_suspicious_url_patterns(url1, url2)
        }
    
    def _check_domain_match(self, url1, url2):
        try:
            domain1 = self._extract_domain(url1) # í˜„ì¬ URL
            domain2 = self._extract_domain(url2) # ê¸°ì¤€ URL
            if domain1 == domain2:
                return {'match': True, 'type': 'exact'}
            main1 = '.'.join(domain1.split('.')[-2:])
            main2 = '.'.join(domain2.split('.')[-2:])
            if main1 == main2:
                return {'match': True, 'type': 'main_domain'}
            return {'match': False, 'type': 'different'}
        except Exception:
            return {'match': False, 'type': 'error'}
    
    def _url_levenshtein_similarity(self, url1, url2):
        distance = Levenshtein.distance(url1.lower(), url2.lower())
        max_len = max(len(url1), len(url2))
        return 1 - (distance / max_len) if max_len > 0 else 0.0
    
    def _check_suspicious_url_patterns(self, url1, url2):
        suspicious = []
        if re.search(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', url1):
            suspicious.append('IP ì£¼ì†Œ ì§ì ‘ ì‚¬ìš©')
        if len(url1) > 100:
            suspicious.append('ë¹„ì •ìƒì ìœ¼ë¡œ ê¸´ URL')
        suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.gq', '.xyz', '.top']
        if any(url1.endswith(tld) for tld in suspicious_tlds):
            suspicious.append(f'ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ìµœìƒìœ„ ë„ë©”ì¸ ì‚¬ìš©')
        url2_domain = self._extract_domain(url2)
        url1_domain = self._extract_domain(url1)
        if url1_domain != url2_domain and Levenshtein.distance(url1_domain, url2_domain) <= 2:
            suspicious.append('ìœ ì‚¬ ë„ë©”ì¸ íƒì§€ (íƒ€ì´í¬ìŠ¤ì¿¼íŒ… ê°€ëŠ¥ì„±)')
        return suspicious

    # â˜… [ì‹ ê·œ] 4-1. ë¡œê·¸ì¸ í¼ URL ê³ ë„í™”
    def analyze_login_forms(self, html, base_url):
        soup = BeautifulSoup(html, 'html.parser')
        base_domain = self._extract_domain(base_url)
        suspicious_forms = []
        password_inputs = soup.find_all('input', {'type': 'password'})
        
        for input_field in password_inputs:
            form = input_field.find_parent('form')
            if not form: continue

            action_url_raw = form.get('action', '')
            action_url_abs = urljoin(base_url, action_url_raw)
            action_domain = self._extract_domain(action_url_abs)

            # í˜„ì¬ URL ë„ë©”ì¸ê³¼ í¼ ì•¡ì…˜ ë„ë©”ì¸ì´ ë‹¤ë¥¼ ê²½ìš°
            if base_domain and action_domain and base_domain != action_domain:
                suspicious_forms.append({
                    'action_url': action_url_abs,
                    'reason': f"ë¡œê·¸ì¸ í¼ì´ ì™¸ë¶€ ë„ë©”ì¸({action_domain})ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤."
                })
        
        return {
            'total_login_forms': len(password_inputs),
            'suspicious_forms': suspicious_forms
        }
    
    # ==================== 5. ë‚´ë¶€ ë§í¬ URL ì¶”ì¶œ ë° ê²€ì‚¬ (ê³ ë„í™”) ====================
    
    def analyze_internal_links(self, html, base_url):
        soup = BeautifulSoup(html, 'html.parser')
        links = soup.find_all('a', href=True)
        analysis = {'total_links': 0, 'internal_links': [], 'external_links': [], 'suspicious_links': []}
        try:
            base_domain = self._extract_domain(base_url)
        except Exception:
            base_domain = ""

        for link in links:
            href = link.get('href', '').strip()
            
            # â˜… [ìˆ˜ì •] javascript:, mailto:, #ë§Œ ìˆëŠ” ë§í¬ ë“±ì€ ìœ íš¨ ë§í¬ì—ì„œ ì œì™¸
            if not href or href.startswith(('javascript:', 'mailto:', '#', 'tel:')):
                continue
            
            analysis['total_links'] += 1 # ìœ íš¨ ë§í¬ ì¹´ìš´íŠ¸
            absolute_url = urljoin(base_url, href)
            link_domain = self._extract_domain(absolute_url)
            
            link_info = {
                'url': absolute_url,
                'text': link.get_text(strip=True)[:50],
                'is_internal': False
            }

            if base_domain and link_domain == base_domain:
                link_info['is_internal'] = True
                analysis['internal_links'].append(link_info)
            else:
                analysis['external_links'].append(link_info)
            
            suspicion = self._is_suspicious_link(absolute_url, link.get_text(strip=True))
            if suspicion['is_suspicious']:
                link_info['reason'] = suspicion['reason']
                analysis['suspicious_links'].append(link_info)
        return analysis
    
    def _is_suspicious_link(self, url, text):
        """ â˜… [ìˆ˜ì •] ë§í¬ê°€ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ì§€ í™•ì¸ (ê³ ë„í™”) """
        reasons = []
        
        # í…ìŠ¤íŠ¸ì™€ ì‹¤ì œ URL ë„ë©”ì¸ ë¶ˆì¼ì¹˜
        if text and 'http' in text.lower():
            try:
                text_domain = self._extract_domain(text.strip().split()[0]) # í…ìŠ¤íŠ¸ ì¤‘ ì²« ë‹¨ì–´
                url_domain = self._extract_domain(url)
                if text_domain and url_domain and text_domain != url_domain:
                    reasons.append(f"ë§í¬ í…ìŠ¤íŠ¸ ë„ë©”ì¸({text_domain})ê³¼ ì‹¤ì œ ë„ë©”ì¸({url_domain}) ë¶ˆì¼ì¹˜")
            except Exception:
                pass # URL íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ

        shorteners = ['bit.ly', 'goo.gl', 't.co', 'tinyurl.com', 'ow.ly']
        if any(shortener in url for shortener in shorteners):
            reasons.append('ë‹¨ì¶• URL ì‚¬ìš©')
        
        return {
            'is_suspicious': bool(reasons),
            'reason': ', '.join(reasons) if reasons else 'ì•Œ ìˆ˜ ì—†ëŠ” ì´ìœ '
        }
    
    # ==================== 6. iframe í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ê²€ì‚¬ ====================
    
    def analyze_iframes(self, html, current_url):
        soup = BeautifulSoup(html, 'html.parser')
        iframes = soup.find_all('iframe')
        analysis = {'total_iframes': len(iframes), 'whitelisted_iframes': [], 'suspicious_iframes': [], 'risk_level': 'low'}
        
        for idx, iframe in enumerate(iframes):
            iframe_info = self._analyze_single_iframe(iframe, current_url, idx)
            if iframe_info['is_whitelisted']:
                analysis['whitelisted_iframes'].append(iframe_info)
            else:
                analysis['suspicious_iframes'].append(iframe_info)
        
        suspicious_count = len(analysis['suspicious_iframes'])
        if suspicious_count == 0:
            analysis['risk_level'] = 'low'
        elif suspicious_count <= 2:
            analysis['risk_level'] = 'medium'
        else:
            analysis['risk_level'] = 'high'
        return analysis
    
    def _analyze_single_iframe(self, iframe, current_url, index):
        src = iframe.get('src', '')
        width = iframe.get('width', '')
        height = iframe.get('height', '')
        style = iframe.get('style', '')
        iframe_info = {'index': index, 'src': src, 'is_whitelisted': False, 'match_info': {}, 'warnings': []}
        
        if src:
            whitelist_check = self.is_url_whitelisted(src)
            iframe_info['is_whitelisted'] = whitelist_check['is_whitelisted']
            iframe_info['match_info'] = whitelist_check
            if not whitelist_check['is_whitelisted']:
                iframe_info['warnings'].append("âš ï¸ ê²½ê³ : ê²€ì¦ë˜ì§€ ì•Šì€ URLì…ë‹ˆë‹¤.")
        else:
            iframe_info['warnings'].append("âš ï¸ src ì†ì„±ì´ ì—†ëŠ” iframeì…ë‹ˆë‹¤.")
        
        if self._is_hidden_iframe(width, height, style):
            iframe_info['warnings'].append("ğŸ”´ ìˆ¨ê²¨ì§„ iframeì´ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            iframe_info['is_whitelisted'] = False
        if src and self._has_suspicious_src_pattern(src):
            iframe_info['warnings'].append("ğŸ”´ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ src íŒ¨í„´ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            iframe_info['is_whitelisted'] = False
        
        return iframe_info
    
    def _extract_domain(self, url):
        try:
            parsed = urlparse(url)
            return parsed.netloc
        except Exception: return ""
    
    def _is_hidden_iframe(self, width, height, style):
        try:
            if width and int(re.sub(r'[^\d]', '', str(width))) <= 1: return True
            if height and int(re.sub(r'[^\d]', '', str(height))) <= 1: return True
        except Exception: pass
        hidden_patterns = [r'display:\s*none', r'visibility:\s*hidden', r'opacity:\s*0']
        return any(re.search(pattern, style, re.IGNORECASE) for pattern in hidden_patterns)
    
    def _has_suspicious_src_pattern(self, src):
        suspicious_patterns = [r'data:text/html', r'javascript:', r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}']
        return any(re.search(pattern, src, re.IGNORECASE) for pattern in suspicious_patterns)
    
    # ==================== 7. ì¢…í•© ìœ„í—˜ë„ í‰ê°€ (CodeBERT ì œì™¸) ====================
    
    def calculate_overall_risk(self, results):
        """ â˜… [ìˆ˜ì •] ì¢…í•© ìœ„í—˜ë„ ê³„ì‚° (CodeBERT í˜ë„í‹° ì œê±°, ë¡œê·¸ì¸ í¼ í˜ë„í‹° ì¶”ê°€) """
        risk_score = 0
        warnings = []
        
        # 1. URL ìœ ì‚¬ë„ (20ì )
        if not results['url_similarity']['domain_match']['match']:
            risk_score += 20
            warnings.append('ë„ë©”ì¸ì´ ì •ìƒ ì‚¬ì´íŠ¸ì™€ ë‹¤ë¦…ë‹ˆë‹¤.')
        if results['url_similarity']['suspicious_patterns']:
            risk_score += 10
            warnings.extend(results['url_similarity']['suspicious_patterns'])
        
        # 2. í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ (20ì )
        text_sim = results['text_similarity']['cosine_similarity']
        if text_sim < 0.5:
            risk_score += 20
            warnings.append(f"í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ê°€ ë‚®ìŠµë‹ˆë‹¤ ({text_sim:.2f})")
        elif text_sim < 0.7:
            risk_score += 10
        
        # 3. DOM êµ¬ì¡° ìœ ì‚¬ë„ (15ì )
        dom_sim = results['dom_similarity']['tree_edit_distance']
        if dom_sim < 0.5:
            risk_score += 15
            warnings.append(f"DOM êµ¬ì¡°ê°€ í¬ê²Œ ë‹¤ë¦…ë‹ˆë‹¤ ({dom_sim:.2f})")
        elif dom_sim < 0.7:
            risk_score += 7
        
        # 4. ì˜ë¯¸/í¼ ë¶„ì„ (25ì )
        semantic = results['semantic_features']['suspicious_features']
        
        # â˜… [ì œê±°] CodeBERT ì‹œë§¨í‹± ìœ ì‚¬ë„ í˜ë„í‹° ì œê±° (ì •ìƒ vs ì •ìƒ ì˜¤ë¥˜ í•´ê²°)
        
        if semantic['has_urgent_keywords']:
            risk_score += 10
            warnings.append('ê¸´ê¸‰ í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.')
        
        # â˜… [ì‹ ê·œ] ë¡œê·¸ì¸ í¼ ë¶„ì„ í˜ë„í‹° (ê³ ë„í™”)
        login_form_analysis = results['login_form_analysis']
        if login_form_analysis['suspicious_forms']:
            risk_score += 20 # (ë†’ì€ í˜ë„í‹°)
            warnings.append(login_form_analysis['suspicious_forms'][0]['reason'])
        
        # 5. ë‚´ë¶€ ë§í¬ ë¶„ì„ (10ì )
        if results['internal_links']['suspicious_links']:
            risk_score += 10
            warnings.append(f"{len(results['internal_links']['suspicious_links'])}ê°œì˜ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë§í¬ê°€ ìˆìŠµë‹ˆë‹¤.")
        
        # 6. iframe ë¶„ì„ (20ì )
        iframe_analysis = results['iframe_analysis']
        if iframe_analysis['risk_level'] == 'medium':
            risk_score += 10
        elif iframe_analysis['risk_level'] == 'high':
            risk_score += 20
        
        # â˜… [ìˆ˜ì •] ìœ„í—˜ë„ ì ìˆ˜ ìµœëŒ€ 100ì ìœ¼ë¡œ ì œí•œ
        risk_score = min(risk_score, 100)

        # ìœ„í—˜ë„ ë“±ê¸‰
        if risk_score >= 60:
            level = 'high'
        elif risk_score >= 30:
            level = 'medium'
        else:
            level = 'low'
        
        return {
            'risk_score': risk_score,
            'risk_level': level,
            'warnings': warnings
        }